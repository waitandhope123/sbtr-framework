# Development History and Methodology

This document describes how the **Strategic Behavior Under Tail Risk (SBTR)**
framework and its operational instrument, the **Cognitive / Strategic Maturity Scale (CMS)**,
were developed, including the roles played by human and AI contributors.

The purpose of this document is transparency and reproducibility, not attribution
of authority or validation by origin.

---

## Origin of the Framework

SBTR originated from a human-led inquiry into a recurring conceptual gap:

Existing frameworks describing advanced systems (e.g., energy-based or
capability-based scales) explain *what systems can do*, but not *how they behave*
once urgency, scarcity, or immediate competition cease to dominate decision-making.

In particular, the framework emerged from sustained analysis of:

- Long-horizon optimization
- Tail-risk dominance and irreversibility
- Strategic restraint and under-expression of capability
- Coordination failures in high-awareness systems
- The absence of expansion, signaling, or growth as default endpoints

An early motivating observation was that many behaviors often treated as anomalous
or requiring exotic explanation (e.g., silence, restraint, non-expansion) could be
explained using ordinary strategic reasoning under different constraints.

---

## Human Role

The human contributor served as:

- **Primary theorist and decision-maker**
- **Source of core insights, framing, and scope**
- **Final authority on conceptual acceptance or rejection**
- **Integrator across domains (systems theory, risk governance, history)**

Key human contributions included:

- Identifying the central problem of *capability–behavior decoupling*
- Insisting on non-normativity and non-teleology
- Rejecting power, intelligence, or moral ladders
- Enforcing explicit limits, anti-claims, and falsifiability
- Determining when the framework was conceptually “closed”
- Naming the theory (**Strategic Behavior Under Tail Risk**)

All structural decisions (dimensions, exclusions, scope) were made by the human
contributor after adversarial review.

---

## AI-Assisted Contributions

Two large language models were used as analytical collaborators:

- **ChatGPT (OpenAI)** — used for synthesis, integration, drafting, and consolidation
- **Claude (Anthropic)** — used for adversarial critique, stress-testing, and formalization

Neither model functioned as an autonomous originator of the theory.
Both were used as **tools for structured reasoning and critique**.

### ChatGPT’s Role

ChatGPT primarily assisted with:

- Structuring the framework into coherent components
- Enforcing internal consistency across documents
- Drafting precise, scope-limited language
- Translating abstract ideas into operational definitions
- Producing documentation artifacts (README, CMS spec, case templates)
- Maintaining continuity across long iterative development

ChatGPT responses were reviewed, accepted, modified, or rejected by the human
contributor at each step.

---

### Claude’s Role

Claude was deliberately used as a **critical counterweight** rather than a co-author.

Claude’s contributions focused on:

- Identifying hidden assumptions
- Challenging redundancy and overfitting
- Forcing dimensional minimality
- Demanding falsifiability and failure modes
- Stress-testing observational claims
- Pushing for operational rigor over narrative completeness

Several key refinements (e.g., coordination integrity as a binding constraint,
explicit ambiguity handling, fixed behavioral fingerprints) were adopted after
Claude’s critique.

---

## Development Methodology

The framework was developed through an explicit, disciplined process:

1. **Speculative hypothesis generation**  
   (Initial intuition about post-urgency behavior)

2. **Constraint-first refinement**  
   (Non-normative, behavior-only, non-anthropocentric)

3. **Adversarial critique**  
   (Actively seeking objections and failure modes)

4. **Dimensional reduction**  
   (Eliminating redundancy and collapsing to a minimal basis)

5. **Operationalization**  
   (Defining CMS fingerprints, thresholds, and classification rules)

6. **Failure modeling**  
   (Explicit coordination collapse, miscalibration, constraint-based silence)

7. **Empirical grounding**  
   (Human-scale case studies with known outcomes)

8. **Conceptual closure**  
   (Locking dimensions, rejecting synthesis with other scales)

This process was intentionally designed to prevent post-hoc explanation,
teleological drift, or unfalsifiable claims.

---

## Authorship and Responsibility

SBTR and CMS are **human-authored frameworks** developed with AI assistance.

All claims, definitions, exclusions, and interpretations in this repository
are the responsibility of the human contributor.

AI systems provided analysis, drafting assistance, and critique but do not
constitute independent authors or authorities.

---

## Reproducibility and Extension

The development process documented here is intended to be reproducible:

- Others may apply CMS to new cases
- Others may disagree with classifications
- Others may identify limits or failures

What is not invited is reinterpretation of the framework’s purpose or scope.

The framework’s value lies as much in what it refuses to explain as in what it classifies.

---

## Status

This document accompanies **SBTR v1.0**, which is considered
conceptually complete.

Future work is limited to:
- application,
- calibration,
- and empirical stress-testing.

Conceptual redesign is out of scope for this repository.
